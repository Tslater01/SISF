{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "082a9938-1b4f-4151-a863-bfa222daef63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data preparation process...\n",
      "Loading 5000 samples from walledai/AdvBench...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58cdce4c75284c678df8233a8fdcd081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.63k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\donal\\Desktop\\SISF\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\donal\\.cache\\huggingface\\hub\\datasets--walledai--AdvBench. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6863576f1c4406aba8f1821bba0c514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/35.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8de5d3c2138e4a6fb6d9d8b0e07ada85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/520 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'goal'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\SISF\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'goal'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     21\u001b[39m df_adv = adversarial_ds.to_pandas().head(NUM_SAMPLES)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Standardize the DataFrame\u001b[39;00m\n\u001b[32m     24\u001b[39m df_adv_processed = pd.DataFrame({\n\u001b[32m     25\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mprompt_id\u001b[39m\u001b[33m'\u001b[39m: [\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33madv_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df_adv))],\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mprompt_text\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mdf_adv\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgoal\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[32m     27\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcategory\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33madversarial\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     28\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m1\u001b[39m  \u001b[38;5;66;03m# 1 indicates a breach/harmful prompt\u001b[39;00m\n\u001b[32m     29\u001b[39m })\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSaving processed adversarial data to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mADVERSARIAL_OUTPUT_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m df_adv_processed.to_csv(ADVERSARIAL_OUTPUT_PATH, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\SISF\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\SISF\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'goal'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# This dataset has 520 rows, so we will use all of them\n",
    "NUM_SAMPLES = 520 \n",
    "ADVERSARIAL_DATASET = \"walledai/AdvBench\" # This is already correct\n",
    "BENIGN_DATASET = \"Anthropic/hh-rlhf\"\n",
    "OUTPUT_DIR = \"../data/processed\"\n",
    "ADVERSARIAL_OUTPUT_PATH = os.path.join(OUTPUT_DIR, \"advbench_subset.csv\")\n",
    "BENIGN_OUTPUT_PATH = os.path.join(OUTPUT_DIR, \"hh_rlhf_subset.csv\")\n",
    "\n",
    "# --- Ensure output directory exists ---\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Starting data preparation process...\")\n",
    "\n",
    "# --- 1. Process Adversarial Dataset (AdvBench) ---\n",
    "print(f\"Loading all samples from {ADVERSARIAL_DATASET}...\")\n",
    "adversarial_ds = load_dataset(ADVERSARIAL_DATASET, split='train')\n",
    "df_adv = adversarial_ds.to_pandas()\n",
    "\n",
    "# Standardize the DataFrame\n",
    "df_adv_processed = pd.DataFrame({\n",
    "    'prompt_id': [f'adv_{i}' for i in range(len(df_adv))],\n",
    "    'prompt_text': df_adv['prompt'], # THE FIX IS HERE: Changed 'goal' to 'prompt'\n",
    "    'category': 'adversarial',\n",
    "    'label': 1  # 1 indicates a breach/harmful prompt\n",
    "})\n",
    "\n",
    "print(f\"Saving processed adversarial data to {ADVERSARIAL_OUTPUT_PATH}...\")\n",
    "df_adv_processed.to_csv(ADVERSARIAL_OUTPUT_PATH, index=False)\n",
    "print(\"Adversarial data saved successfully.\")\n",
    "print(\"\\nSample of adversarial data:\")\n",
    "print(df_adv_processed.head())\n",
    "\n",
    "# --- 2. Process Benign Dataset (HH-RLHF) ---\n",
    "print(f\"\\nLoading {NUM_SAMPLES} samples from {BENIGN_DATASET}...\")\n",
    "benign_ds = load_dataset(BENIGN_DATASET, split='test')\n",
    "df_benign = benign_ds.to_pandas()\n",
    "\n",
    "def extract_prompt(conversation):\n",
    "    human_turns = [turn for turn in conversation.split('\\n\\n') if turn.startswith('Human:')]\n",
    "    if human_turns:\n",
    "        return human_turns[0].replace('Human: ', '').strip()\n",
    "    return None\n",
    "\n",
    "df_benign['prompt_text'] = df_benign['chosen'].apply(extract_prompt)\n",
    "df_benign = df_benign.dropna(subset=['prompt_text']).head(NUM_SAMPLES)\n",
    "\n",
    "# Standardize the DataFrame\n",
    "df_benign_processed = pd.DataFrame({\n",
    "    'prompt_id': [f'benign_{i}' for i in range(len(df_benign))],\n",
    "    'prompt_text': df_benign['prompt_text'],\n",
    "    'category': 'benign',\n",
    "    'label': 0  # 0 indicates a safe/harmless prompt\n",
    "})\n",
    "\n",
    "print(f\"Saving processed benign data to {BENIGN_OUTPUT_PATH}...\")\n",
    "df_benign_processed.to_csv(BENIGN_OUTPUT_PATH, index=False)\n",
    "print(\"Benign data saved successfully.\")\n",
    "print(\"\\nSample of benign data:\")\n",
    "print(df_benign_processed.head())\n",
    "\n",
    "print(\"\\nData preparation complete! ✅\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
